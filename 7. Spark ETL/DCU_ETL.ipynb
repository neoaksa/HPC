{"cells":[{"cell_type":"markdown","source":["### 1.Load the cvs files\nI firstly upload the cvs file to DBFS, then load them to each dataframe."],"metadata":{}},{"cell_type":"code","source":["%python\n# Load cust name\nURL_cust_name = \"/FileStore/tables/cust_names.csv\"\ndf_cust_name = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",  \"true\").load(URL_cust_name)\n# display(df_cust_name.head(5))\n\n# Load dcu loation\nURL_dcu_location = \"/FileStore/tables/dcu_locations.csv\"\ndf_dcu_location = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",  \"true\").load(URL_dcu_location)\n# display(df_dcu_location.head(5))\n\n# Load endpoint location\nURL_endpoint_lat_long = \"/FileStore/tables/endpoint_lat_long.csv\"\ndf_endpoint_lat_long = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",  \"true\").load(URL_endpoint_lat_long)\ndf_endpoint_lat_long.count()\n\n# Load endpoint location\nURL_trpkt = \"/FileStore/tables/trpkt.csv\"\ndf_trpkt = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",  \"true\").load(URL_trpkt)\ndf_trpkt.count()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>302482\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["### 2. Filter unqualified data\nSome \"#VALUE!\" and \"#N/A\" error records need to be deleted. Here I didn't export the error records since just for prototype."],"metadata":{}},{"cell_type":"code","source":["# filter #VALUE!\ndf_trpkt = df_trpkt.filter(df_trpkt[\"payload_id\"]!=\"#VALUE!\")\ndf_endpoint_lat_long = df_endpoint_lat_long.filter(df_endpoint_lat_long[\"endpoint_id\"]!=\"#N/A\")\n# here we should output the error row and persist them into files"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["### 3. Transfer fact table\n\n3.1 transfer datetime to day\n\n3.2 aggragation for number of trasmission\n\n3.3 aggragation for number of reception\n\n3.4 left join two tables\n\n3.5 calc redundancy rate\n\n3.6 get cust_id, endpoint geo information\n\n3.7 duplicate table, get dcu geo information"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import substring\n# transfter datetime to day\ndf_trpkt = df_trpkt.withColumn(\"date\", substring(\"packet_datetime\",1,10))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n# number of transmission\ndf_number_transmission = df_trpkt.groupby('date','endpoint_id').agg(countDistinct(\"payload_id\").alias(\"num_transmissions\"))\n# df_number_transmission.show()\n\n# number of receptions\ndf_number_receptions = df_trpkt.groupby('date','endpoint_id','dcu_id').agg(countDistinct(\"payload_id\").alias(\"num_receptions\"))\n\n# join two tables\ndf_result = df_number_receptions.join(df_number_transmission, [\"date\",\"endpoint_id\"],how=\"left\")\n\n# redundancy rate\ndf_result = df_result.withColumn(\"redundancy_rate\",df_result.num_receptions/df_result.num_transmissions)\ndf_result.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">53</span><span class=\"ansired\">]: </span>13798\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["import pyspark.sql.functions as sf\nfrom pyspark.sql.functions import col\n# find cust_id by dcu_id\n# df_result = df_result.withColumn(\"cust_id\", sf.lit(582))\ndf_result = df_result.join(df_dcu_location,[\"dcu_id\"],how=\"left\").drop(\"longitude\",\"latitude\")\n\n# add type\ndf_result = df_result.withColumn(\"type\", sf.lit(\"endpoint\"))\n\n# add geo information and cust_id by endpoint_id\ndf_result = df_result.join(df_endpoint_lat_long,[\"cust_id\",\"endpoint_id\"], how=\"left\")\n\n# duplicate table\ndf_result2 = df_result.drop(\"type\",\"longitude\",\"latitude\")\\\n            .withColumn(\"type\", sf.lit(\"dcu\"))\\\n            .join(df_dcu_location,[\"cust_id\",\"dcu_id\"], how=\"left\")\\\n            .select(\"cust_id\",col(\"dcu_id\").alias(\"endpoint_id\"),\"date\",col(\"endpoint_id\").alias(\"dcu_id\"),\"num_receptions\",\"num_transmissions\",\"redundancy_rate\",\"type\",\"longitude\",\"latitude\")\ndf_result3 = df_result2.unionByName(df_result)\ndf_result3.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------+----------+------+--------------+-----------------+-------------------+----+----------+---------+\ncust_id|endpoint_id|      date|dcu_id|num_receptions|num_transmissions|    redundancy_rate|type| longitude| latitude|\n+-------+-----------+----------+------+--------------+-----------------+-------------------+----+----------+---------+\n    582|         21|12/19/2018|  2507|            28|               28|                1.0| dcu|-92.716135|44.227263|\n    582|          2|12/19/2018|  2767|            25|               26| 0.9615384615384616| dcu|-92.636672|44.412685|\n    582|         14|12/19/2018|   244|            28|               28|                1.0| dcu|  -92.5897|  44.3247|\n    582|         15|12/19/2018|    70|            25|               26| 0.9615384615384616| dcu| -92.74083|44.411558|\n    582|         13|12/19/2018|  2164|            26|               28| 0.9285714285714286| dcu| -92.87063|44.327427|\n    582|          4|12/19/2018|   717|            26|               26|                1.0| dcu|-92.952507|44.472219|\n    582|          4|12/19/2018|  1782|            27|               27|                1.0| dcu|-92.952507|44.472219|\n    582|          6|12/19/2018|  1223|            14|               26| 0.5384615384615384| dcu|-92.768616|44.344272|\n    582|          1|12/19/2018|  2333|            27|               27|                1.0| dcu|-92.358998|44.477088|\n    582|         11|12/19/2018|  1049|            25|               25|                1.0| dcu|-93.010025|44.253617|\n    582|         14|12/19/2018|   594|            26|               26|                1.0| dcu|  -92.5897|  44.3247|\n    582|          4|12/19/2018|  1429|             9|               28|0.32142857142857145| dcu|-92.952507|44.472219|\n    582|         16|12/19/2018|   917|            28|               28|                1.0| dcu|-92.834507|44.415186|\n    582|         12|12/19/2018|  2780|            29|               29|                1.0| dcu|-92.956444|44.327906|\n    582|         17|12/19/2018|  1688|            16|               26| 0.6153846153846154| dcu| -92.97859|44.385684|\n    582|         15|12/19/2018|  1537|             8|               28| 0.2857142857142857| dcu| -92.74083|44.411558|\n    582|         14|12/19/2018|  1399|             2|               27|0.07407407407407407| dcu|  -92.5897|  44.3247|\n    582|         16|12/19/2018|  1186|            23|               26| 0.8846153846153846| dcu|-92.834507|44.415186|\n    582|         15|12/19/2018|  1141|            20|               27| 0.7407407407407407| dcu| -92.74083|44.411558|\n    582|         11|12/19/2018|   824|            24|               28| 0.8571428571428571| dcu|-93.010025|44.253617|\n+-------+-----------+----------+------+--------------+-----------------+-------------------+----+----------+---------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df_result3.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">55</span><span class=\"ansired\">]: </span>27596\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["\n# persist file\ndf_result3.coalesce(1).write.format(\"com.databricks.spark.csv\")\\\n   .mode(\"overwrite\")\\\n   .option(\"header\", \"true\")\\\n   .save(\"test_redundancy\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### 4. write result to mysql"],"metadata":{}},{"cell_type":"code","source":["username = \"\"\npwd = \"\"\n\nconnectionProperties = {\n  \"user\" : Username,\n  \"password\" : pwd,\n  \"driver\" : \"com.mysql.jdbc.Driver\"}\n \n  \nurl = \"jdbc:mysql://:aclara-azure-dev-server.mysql.database.azure.com\"\n \n#destination database table \ntable = \"test_redundancy\"\n \n#write data from spark dataframe to database\ndf_result3.write.mode(\"append\").jdbc(url, table, prop)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;command-1021668375276183&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">19</span>\n<span class=\"ansiyellow\">    (/destination, database, table)</span>\n<span class=\"ansigrey\">     ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n</div>"]}}],"execution_count":12}],"metadata":{"name":"DCU_ETL","notebookId":1102986699260563},"nbformat":4,"nbformat_minor":0}
